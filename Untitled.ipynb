{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veri Tipleri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark üzerinde halihazıda temel işlemleri bitirdik dataFrame'ler üzerindeki temel işlemlere gelmeden spark üzerindeki diğer veri tiplerine bir göz atalım."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "WorldCupData=spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(\"WorldCupMatches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Year,IntegerType,true),StructField(Datetime,StringType,true),StructField(Stage,StringType,true),StructField(Stadium,StringType,true),StructField(City,StringType,true),StructField(Home Team Name,StringType,true),StructField(Home Team Goals,IntegerType,true),StructField(Away Team Goals,IntegerType,true),StructField(Away Team Name,StringType,true),StructField(Win conditions,StringType,true),StructField(Attendance,IntegerType,true),StructField(Half-time Home Goals,IntegerType,true),StructField(Half-time Away Goals,IntegerType,true),StructField(Referee,StringType,true),StructField(Assistant 1,StringType,true),StructField(Assistant 2,StringType,true),StructField(RoundID,IntegerType,true),StructField(MatchID,IntegerType,true),StructField(Home Team Initials,StringType,true),StructField(Away Team Initials,StringType,true)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WorldCupData.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------+------------+----------+-----------+\n",
      "| Yıl|  Evsahibi|EvsahibiGol|DeplasmanGol| Deplasman|      sehir|\n",
      "+----+----------+-----------+------------+----------+-----------+\n",
      "|1930|    France|          4|           1|    Mexico|Montevideo |\n",
      "|1930|       USA|          3|           0|   Belgium|Montevideo |\n",
      "|1930|Yugoslavia|          2|           1|    Brazil|Montevideo |\n",
      "|1930|   Romania|          3|           1|      Peru|Montevideo |\n",
      "|1930| Argentina|          1|           0|    France|Montevideo |\n",
      "|1930|     Chile|          3|           0|    Mexico|Montevideo |\n",
      "|1930|Yugoslavia|          4|           0|   Bolivia|Montevideo |\n",
      "|1930|       USA|          3|           0|  Paraguay|Montevideo |\n",
      "|1930|   Uruguay|          1|           0|      Peru|Montevideo |\n",
      "|1930|     Chile|          1|           0|    France|Montevideo |\n",
      "|1930| Argentina|          6|           3|    Mexico|Montevideo |\n",
      "|1930|    Brazil|          4|           0|   Bolivia|Montevideo |\n",
      "|1930|  Paraguay|          1|           0|   Belgium|Montevideo |\n",
      "|1930|   Uruguay|          4|           0|   Romania|Montevideo |\n",
      "|1930| Argentina|          3|           1|     Chile|Montevideo |\n",
      "|1930| Argentina|          6|           1|       USA|Montevideo |\n",
      "|1930|   Uruguay|          6|           1|Yugoslavia|Montevideo |\n",
      "|1930|   Uruguay|          4|           2| Argentina|Montevideo |\n",
      "|1934|   Austria|          3|           2|    France|     Turin |\n",
      "|1934|   Hungary|          4|           2|     Egypt|    Naples |\n",
      "+----+----------+-----------+------------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,struct,array,concat,lit\n",
    "df=WorldCupData.selectExpr(\"Year as Yıl\",\"`Home team name` as Evsahibi\",\"`Home Team Goals` as EvsahibiGol\"\\\n",
    "                           ,\"`Away Team Goals` as DeplasmanGol\",\"`Away Team Name` as Deplasman\",\"City as sehir\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "şimdi gördüğünüz üzere çok fazla parametre tutan dataframimizi basit bir transformation ile daha ufak üzerinde daha rahat antrenman yapabileceğimiz bir hale getirdirdik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Yıl: integer (nullable = true)\n",
      " |-- Evsahibi: string (nullable = true)\n",
      " |-- EvsahibiGol: integer (nullable = true)\n",
      " |-- DeplasmanGol: integer (nullable = true)\n",
      " |-- Deplasman: string (nullable = true)\n",
      " |-- sehir: string (nullable = true)\n",
      "\n",
      "+----+--------------+-----------+------------+-----------------+-----------+\n",
      "| Yıl|      Evsahibi|EvsahibiGol|DeplasmanGol|        Deplasman|      sehir|\n",
      "+----+--------------+-----------+------------+-----------------+-----------+\n",
      "|1938|   Switzerland|          1|           1|          Germany|     Paris |\n",
      "|1938|       Hungary|          6|           0|Dutch East Indies|     Reims |\n",
      "|1938|        France|          3|           1|          Belgium|  Colombes |\n",
      "|1938|          Cuba|          3|           3|          Romania|  Toulouse |\n",
      "|1938|         Italy|          2|           1|           Norway|Marseilles |\n",
      "|1938|        Brazil|          6|           5|           Poland|Strasbourg |\n",
      "|1938|Czechoslovakia|          3|           0|      Netherlands|  Le Havre |\n",
      "|1938|          Cuba|          2|           1|          Romania|  Toulouse |\n",
      "|1938|   Switzerland|          4|           2|          Germany|     Paris |\n",
      "|1938|        Brazil|          1|           1|   Czechoslovakia|  Bordeaux |\n",
      "|1938|       Hungary|          2|           0|      Switzerland|     Lille |\n",
      "|1938|        Sweden|          8|           0|             Cuba|   Antibes |\n",
      "|1938|         Italy|          3|           1|           France|  Colombes |\n",
      "|1938|        Brazil|          2|           1|   Czechoslovakia|  Bordeaux |\n",
      "|1938|       Hungary|          5|           1|           Sweden|     Paris |\n",
      "|1938|         Italy|          2|           1|           Brazil|Marseilles |\n",
      "|1938|        Brazil|          4|           2|           Sweden|  Bordeaux |\n",
      "|1938|         Italy|          4|           2|          Hungary|  Colombes |\n",
      "+----+--------------+-----------+------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.where(\"yıl==1938\").show()#basit bir where kullanımı"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gelin birde sutün üzerinde nasıl kısıtlamalar yapacağımıza bakalım"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi alttaki kod öbeğini anlatalım şehir filtresi için 'Sao' kelimesini içersinde 1 ve ya daha fazla barındıran satırları.\n",
    "<p>Ev sahibi filtresi içinde aynı durum geçerli ve son filtre olan gol filtresi ise 1 deplasmanda atılan gollerin 1 ve ya birden büyük olduğu durumlardaki satırları getirecek. \n",
    "   <p> ve en son where komutu içerisinde hepsini rahatlıkla kullanabileceğimizi görüyorsunuz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+------------+---------+----------+\n",
      "| Yıl|Evsahibi|EvsahibiGol|DeplasmanGol|Deplasman|     sehir|\n",
      "+----+--------+-----------+------------+---------+----------+\n",
      "|1950|  Sweden|          3|           2|    Italy|Sao Paulo |\n",
      "+----+--------+-----------+------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "sehirFilter=instr(df.sehir,\"Sao\")>=1\n",
    "evSahibiFilter=instr(df.Evsahibi,\"Swe\")>=1\n",
    "golFilter=col(\"DeplasmanGol\")>1\n",
    "df.where(sehirFilter & evSahibiFilter & golFilter).show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+------------+---------+----------+--------------+\n",
      "| Yıl|Evsahibi|EvsahibiGol|DeplasmanGol|Deplasman|     sehir|ConditionState|\n",
      "+----+--------+-----------+------------+---------+----------+--------------+\n",
      "|1950|  Sweden|          3|           2|    Italy|Sao Paulo |          true|\n",
      "+----+--------+-----------+------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conditions=sehirFilter & evSahibiFilter & golFilter\n",
    "df.withColumn(\"ConditionState\",conditions).where(\"ConditionState==true\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir yukarıda gördüğünüz kod parçası ise yıkarıda oluşturduğumuz filtreleri sutün üzerinde tekrar uygulayarak her sutündaki ilgili şarta göre boolean değer olarak ek bir sutün haline getirebiliriz yani istediğimiz şartı sağlayan sutünları boolean bir değerle etiketleyebiliriz.\n",
    "Son bir ekleme eğer verinizin içersinde null değerler varsa onu eqNullSafe(\"None\") ile düzeltebilirsiniz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### nümerik ve string data işlemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+------------+-----------+-----------+---------+\n",
      "| Yıl|Evsahibi|EvsahibiGol|DeplasmanGol|  Deplasman|      sehir|ToplamGol|\n",
      "+----+--------+-----------+------------+-----------+-----------+---------+\n",
      "|1938|  Brazil|          6|           5|     Poland|Strasbourg |       11|\n",
      "|1954| Hungary|          8|           3| Germany FR|     Basel |       11|\n",
      "|1954| Austria|          7|           5|Switzerland|  Lausanne |       12|\n",
      "|1982| Hungary|         10|           1|El Salvador|     Elche |       11|\n",
      "+----+--------+-----------+------------+-----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "toplamGol=col(\"EvsahibiGol\")+col(\"DeplasmanGol\")\n",
    "df.select(\"*\",toplamGol.alias(\"ToplamGol\")).where(\"toplamGol>10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gördüğünüz gibi spark üzerindeki iki sutün eğer tipleri uygunsa üzerinde rahatlıkla 4 işlem yapmaya müsait hale geliyor,yukarıda iki col toplamıda bir col olacağıundan aşagıdaki select attığımız zaman ilgili data frame toplam gol sutünuna sahip olacaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "şimdi aşağıda hızlaca sütunlardaki stringler üzerindeki string manipülasyonlarını hızlıca örneklerle gösterip geçicem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|upper(Deplasman)|\n",
      "+----------------+\n",
      "|          MEXICO|\n",
      "|         BELGIUM|\n",
      "|          BRAZIL|\n",
      "+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import  lit,ltrim,rtrim,rpad,lpad,trim,upper\n",
    "df.select(upper(df.Deplasman)).show(3)#ltrim,trim,rpad,lpad sağ ve ya soldan boşluk ekleme ve kesmeler için kullanılır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En son terki diyar etmeden önce string işlemleri ile ilgili ufak bir regular expression örneği verip kendimize sövdürmeyelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|    x|Evsahibi|\n",
      "+-----+--------+\n",
      "|Antep|  France|\n",
      "|  USA|     USA|\n",
      "+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import  regexp_replace\n",
    "reg_string=\"France\"\n",
    "df.select(regexp_replace(col(\"Evsahibi\"),reg_string,\"Antep\").alias(\"x\"),col(\"Evsahibi\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ilgili daha fazla fonksiyonlar için https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/sql/functions.html göz atabilirsiniz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Complex veri tipleri ile çalışma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark üzerinde kullanabileceğimiz 3 farklı  komplex veri tipi vardır.\n",
    "bunlar:structs,arrays,maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struct'lar DataFrame içinde dataFrame olarak düşünülebilinir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "structEx=df.selectExpr(\"struct(EvsahibiGol,DeplasmanGol) as skor\",\"*\")\n",
    "#gördüğünüz üzere bir sql sorgusu ile bir struct oluştrduk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array'ler ise bazı fonksiyonlar ile sütunlardan kendileri olulurlar mesela gelin şehir sütununu birer \"split\" fonksiyonuyla birer arraye dönüştürelim.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      split(sehir, )|\n",
      "+--------------------+\n",
      "|[M, o, n, t, e, v...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+\n",
      "|size(split(sehir, ))|\n",
      "+--------------------+\n",
      "|                  12|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split,size\n",
    "df.select(split(col(\"sehir\"),\"\")).show(1)\n",
    "df.select(size(split(col(\"sehir\"),\"\"))).show(1)\n",
    "#diğer array fonksiyonlarıda aynı şekilde size gibi kullanarak deneyebilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi gelelim belkide spark üzerinde yapacağımız en kritik şeye kullanıcı tanımlı fonksiyonlar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF(user definied function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "şimdi gelelim sparkın bu alandaki faydalarına eğer fonksiyonları pyhton ile yazarsanız spark bir\n",
    "tane pyhton processesi oluşturur işci node üzerinde ve tüm verileri pyhton'ın anlayacağı şekilde serialize eder\n",
    "sonra pyhton kodununda satır satır çalıştırır sonra sonucu JVM'e dönderir \n",
    "ve spark pyhton processeine giren veriyi kontrol edemeyeceği için işci node'lar üzerinde hataya sürükleyebilir bu yüzden user defined fonksiyonlarımızı daha çok scala ve java dilinde kullanırsak verimliliği arttırabiliriz.<p>Aşağıdaki kod öbeğindede görebileceğiniz gibi bir pyhton fonksiyonundan hiçbir farkı yok biz fonksiyona bir sütun gönderiyoruz o ise tüm satırlardaki değerleri 3 ile üssü alınmış bir sutün dönderiyor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|POWER((DeplasmanGol + Evsahibigol), 3)|\n",
      "+--------------------------------------+\n",
      "|                                 125.0|\n",
      "|                                  27.0|\n",
      "|                                  27.0|\n",
      "|                                  64.0|\n",
      "|                                   1.0|\n",
      "|                                  27.0|\n",
      "|                                  64.0|\n",
      "|                                  27.0|\n",
      "|                                   1.0|\n",
      "|                                   1.0|\n",
      "|                                 729.0|\n",
      "|                                  64.0|\n",
      "|                                   1.0|\n",
      "|                                  64.0|\n",
      "|                                  64.0|\n",
      "|                                 343.0|\n",
      "|                                 343.0|\n",
      "|                                 216.0|\n",
      "|                                 125.0|\n",
      "|                                 216.0|\n",
      "+--------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfDf=spark.range(5).toDF(\"data\")\n",
    "def power3(value):\n",
    "    return value**3\n",
    "\n",
    "df.select(power3(col(\"DeplasmanGol\")+col(\"Evsahibigol\"))).show()\n",
    "#Yukarıda ki fonksiyon da gördüğünüz gibi bir sütunu tamamıyla alıp değeri dönderebiliyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGGREGATİONS(Toplama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation aslında gruplama,toplama verileri combine etme şeklinde düşünülenlebilinir.Mesela veriyi groupBy ile gruplama bir aggregationdur.spark üzerinde bize bu yetkinliği veren window,grouping set,rollup,cuybe gibi methodlar vardır şimdi birer birer bunların neler olduğuna bakalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4572"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()#en basit aggregation foksiyonudur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tüm aggregation fonksiyonları için org.apache.spark.sql.functions package 'e gidebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#COUNT\n",
    "#When count is performing as count(*) it count nul values \n",
    "#also but if you count spesific colum it will not count the null values.\n",
    "from pyspark.sql.functions import count,countDistinct,approxCountDistinct,first\\\n",
    ",last,min,max,sum,sumDistinct,avg,var_pop,stddev,kurtosis,skewness,corr,covar_pop,covar_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|count(sehir)|\n",
      "+------------+\n",
      "|         852|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count(\"sehir\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT sehir)|\n",
      "+---------------------+\n",
      "|                  151|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#bu ise bize dünya kupalarının şu ana kadar kaç farklı şehirde oynandığı bilgisini veriyor.\n",
    "df.select(countDistinct(\"sehir\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|approx_count_distinct(sehir)|\n",
      "+----------------------------+\n",
      "|                         155|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#çok büyük verilerle çalışacağımız zaman bu distinct count işlemleri çok maliyetli olacağından\n",
    "#işlemi bir yakınsama değerinde alıyoruz,\n",
    "#şöyle düşünebiliriz ilk yüz satırda kaç farklı değer var ise genele uyarla.\n",
    "df.select(approxCountDistinct(\"sehir\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|first(sehir, false)|\n",
      "+-------------------+\n",
      "|        Montevideo |\n",
      "+-------------------+\n",
      "\n",
      "+------------------+\n",
      "|last(sehir, false)|\n",
      "+------------------+\n",
      "|              null|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#firs and last,you can get first and last value from a Df\n",
    "df.select(first(\"sehir\")).show()\n",
    "df.select(last(\"sehir\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|max(EvSahibiGol)|min(DeplasmanGol)|\n",
      "+----------------+-----------------+\n",
      "|              10|                0|\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#min and max :return min and max value from column.\n",
    "df.select(max(\"EvSahibiGol\"),min(\"DeplasmanGol\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|sum(EvSahibiGol)|sum(DeplasmanGol)|\n",
      "+----------------+-----------------+\n",
      "|            1543|              871|\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sum,sumDistinct\n",
    "#ev sahipleri 1543 gol atarken deplasman takımları ise 871 kadar gol atabilmiş.\n",
    "df.select(sum(\"EvSahibiGol\"),sum(\"DeplasmanGol\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "|  avg(EvsahibiGol)|avg(DeplasmanGol)|\n",
      "+------------------+-----------------+\n",
      "|1.8110328638497653|1.022300469483568|\n",
      "+------------------+-----------------+\n",
      "\n",
      "+--------------------+------------------------+\n",
      "|var_pop(EvsahibiGol)|stddev_samp(EvsahibiGol)|\n",
      "+--------------------+------------------------+\n",
      "|   2.589878275915273|      1.6102551385229658|\n",
      "+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#avg,variance,standart deviation\n",
    "#atılan ortalama goller.\n",
    "df.select(avg(\"EvsahibiGol\"),avg(\"DeplasmanGol\")).show()\n",
    "df.select(var_pop(\"EvsahibiGol\"),stddev(\"EvsahibiGol\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|skewness(EvsahibiGol)|kurtosis(EvsahibiGol)|\n",
      "+---------------------+---------------------+\n",
      "|   1.3493586476038122|     2.61650781761948|\n",
      "+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#kısaca şimdilik ortalama değere göre  uzaklaşma türüne göre çarpıklık ve basıklık olarak bilin.\n",
    "\n",
    "df.select(skewness(\"EvsahibiGol\"),kurtosis(\"EvsahibiGol\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+------------------------------------+\n",
      "|corr(EvsahibiGol, DeplasmanGol)|covar_samp(EvsahibiGol, DeplasmanGol)|covar_pop(EvsahibiGol, DeplasmanGol)|\n",
      "+-------------------------------+-------------------------------------+------------------------------------+\n",
      "|           0.012473988573227762|                 0.021845329714282765|                 0.02181968965593267|\n",
      "+-------------------------------+-------------------------------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Covariance,Correlation:\n",
    "#bunlar ise verileri +1 ile -1 arasında ölçeklendirmek için kullandığımız methodlardır.\n",
    "df.select(corr(\"EvsahibiGol\",\"DeplasmanGol\"),covar_samp(\"EvsahibiGol\",\"DeplasmanGol\"),covar_pop(\"EvsahibiGol\",\"DeplasmanGol\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGREGATING TO COMPLEX TYPES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+-----+\n",
      "|             Sehir| yıl|count|\n",
      "+------------------+----+-----+\n",
      "|            Paris |1938|    3|\n",
      "|           Toluca |1970|    4|\n",
      "|            Gijon |1982|    3|\n",
      "|        Nuremberg |2006|    5|\n",
      "|          Hamburg |2006|    5|\n",
      "|        Polokwane |2010|    4|\n",
      "|        Stuttgart |1974|    4|\n",
      "|             Oita |2002|    3|\n",
      "|    Gelsenkirchen |1974|    5|\n",
      "|       Norrk�Ping |1958|    3|\n",
      "|Santiago De Chile |1962|   10|\n",
      "|          Hanover |2006|    5|\n",
      "|         Valencia |1982|    3|\n",
      "|          Leipzig |2006|    5|\n",
      "|       Sunderland |1966|    4|\n",
      "|            Arica |1962|    7|\n",
      "|        Monterrey |1986|    8|\n",
      "|    San Francisco |1994|    6|\n",
      "|           Lugano |1954|    1|\n",
      "|             Rome |1990|    6|\n",
      "+------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#gropings\n",
    "df.groupBy(\"Sehir\",\"yıl\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set,collect_list,expr,count,column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  collect_set(Sehir)| collect_list(Sehir)|\n",
      "+--------------------+--------------------+\n",
      "|[Mendoza , Saint-...|[Montevideo , Mon...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(collect_set(\"Sehir\"),collect_list(\"Sehir\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+----------+\n",
      "|             Sehir|quan|count(Yıl)|\n",
      "+------------------+----+----------+\n",
      "|            Daegu |   4|         4|\n",
      "|            Paris |   9|         9|\n",
      "|            Natal |   4|         4|\n",
      "|    San Francisco |   6|         6|\n",
      "|Santiago De Chile |  10|        10|\n",
      "|       Eskilstuna |   1|         1|\n",
      "|        La Coru�A |   3|         3|\n",
      "|           Bilbao |   3|         3|\n",
      "|           Geneva |   4|         4|\n",
      "|         Le Havre |   1|         1|\n",
      "|           Verona |   4|         4|\n",
      "|             Kobe |   3|         3|\n",
      "|            Solna |   8|         8|\n",
      "|        Liverpool |   5|         5|\n",
      "|          Gwangju |   3|         3|\n",
      "|           Cuiaba |   4|         4|\n",
      "|          Niigata |   3|         3|\n",
      "|      Guadalajara |  17|        17|\n",
      "|           Boston |   6|         6|\n",
      "|           Madrid |   7|         7|\n",
      "+------------------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#grouping with expression\n",
    "df.groupBy(\"Sehir\").agg(count(\"Yıl\").alias(\"quan\"),expr(\"count(Yıl)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------------+\n",
      "| Yıl|  avg(EvsahibiGol)|stddev_pop(DeplasmanGol)|\n",
      "+----+------------------+------------------------+\n",
      "|1990|1.2884615384615385|      1.0348941574672084|\n",
      "|1974|1.3421052631578947|      1.3795097286401221|\n",
      "|2006|           1.34375|      1.0372693644251716|\n",
      "|1978|2.0789473684210527|      0.6299320633466223|\n",
      "|null|              null|                    null|\n",
      "|1934| 2.823529411764706|      0.6655122646461624|\n",
      "|1994|1.5961538461538463|      1.0124958908529371|\n",
      "|2014|            1.2375|      1.5647983096872262|\n",
      "|1930|3.2777777777777777|      0.8258927081843614|\n",
      "|1938| 3.388888888888889|      1.1928283640879938|\n",
      "|1950|3.1363636363636362|      0.8143851303258598|\n",
      "|1966|           2.15625|      0.6959705453537528|\n",
      "|1982|1.8653846153846154|      0.7945357023120615|\n",
      "|1998|           1.53125|      1.0438388809461927|\n",
      "|1970|              2.25|      0.8379429798619952|\n",
      "|1958|2.5142857142857142|      1.0245954901166348|\n",
      "|1954|4.1923076923076925|       1.301455716591433|\n",
      "|2010|            1.1875|        0.95695949986141|\n",
      "|1986|1.4230769230769231|       1.171025949818333|\n",
      "|1962|           2.15625|        0.81967981553775|\n",
      "+----+------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#gropings with map\n",
    "df.groupBy(\"Yıl\").agg(expr(\"avg(EvsahibiGol)\"),expr(\"stddev_pop(DeplasmanGol)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,to_date,desc,dense_rank,rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate=WorldCupData.withColumn(\"date\",to_date(col(\"Datetime\"),\"MM/d/yyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")\n",
    "#data kullanımınıda burada göreb,l,rs,n,z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------+-------+-----------+--------------+---------------+---------------+--------------+--------------+----------+--------------------+--------------------+--------------------+--------------------+-------------------+-------+-------+------------------+------------------+----+\n",
      "|Year|            Datetime|  Stage|Stadium|       City|Home Team Name|Home Team Goals|Away Team Goals|Away Team Name|Win conditions|Attendance|Half-time Home Goals|Half-time Away Goals|             Referee|         Assistant 1|        Assistant 2|RoundID|MatchID|Home Team Initials|Away Team Initials|date|\n",
      "+----+--------------------+-------+-------+-----------+--------------+---------------+---------------+--------------+--------------+----------+--------------------+--------------------+--------------------+--------------------+-------------------+-------+-------+------------------+------------------+----+\n",
      "|1930|13 Jul 1930 - 15:00 |Group 1|Pocitos|Montevideo |        France|              4|              1|        Mexico|              |      4444|                   3|                   0|LOMBARDI Domingo ...|CRISTOPHE Henry (...|REGO Gilberto (BRA)|    201|   1096|               FRA|               MEX|null|\n",
      "+----+--------------------+-------+-------+-----------+--------------+---------------+---------------+--------------+--------------+----------+--------------------+--------------------+--------------------+--------------------+-------------------+-------+-------+------------------+------------------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Grouping Sets\n",
    "dfNoNull=dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")\n",
    "dfNoNull.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rollups:çok yönlü aggregation için ise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`EvsahibiGol`' given input columns: [Sehir, Evsahibi];;\\n'Sort ['EvsahibiGol ASC NULLS FIRST], true\\n+- AnalysisBarrier\\n      +- Project [Sehir#2824, Evsahibi#2825]\\n         +- Aggregate [Sehir#2824, Evsahibi#2825, spark_grouping_id#2821], [Sehir#2824, Evsahibi#2825, sum(cast(EvsahibiGol#90 as bigint)) AS sum(EvsahibiGol)#2820L]\\n            +- Expand [ArrayBuffer(Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#2822, Evsahibi#2823, 0), ArrayBuffer(Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#2822, null, 1), ArrayBuffer(Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, null, null, 3)], [Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#2824, Evsahibi#2825, spark_grouping_id#2821]\\n               +- Project [Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#93 AS Sehir#2822, Evsahibi#89 AS Evsahibi#2823]\\n                  +- Project [Year#10 AS Yıl#88, Home team name#15 AS Evsahibi#89, Home Team Goals#16 AS EvsahibiGol#90, Away Team Goals#17 AS DeplasmanGol#91, Away Team Name#18 AS Deplasman#92, City#14 AS sehir#93]\\n                     +- Relation[Year#10,Datetime#11,Stage#12,Stadium#13,City#14,Home Team Name#15,Home Team Goals#16,Away Team Goals#17,Away Team Name#18,Win conditions#19,Attendance#20,Half-time Home Goals#21,Half-time Away Goals#22,Referee#23,Assistant 1#24,Assistant 2#25,RoundID#26,MatchID#27,Home Team Initials#28,Away Team Initials#29] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1576.sort.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`EvsahibiGol`' given input columns: [Sehir, Evsahibi];;\n'Sort ['EvsahibiGol ASC NULLS FIRST], true\n+- AnalysisBarrier\n      +- Project [Sehir#2824, Evsahibi#2825]\n         +- Aggregate [Sehir#2824, Evsahibi#2825, spark_grouping_id#2821], [Sehir#2824, Evsahibi#2825, sum(cast(EvsahibiGol#90 as bigint)) AS sum(EvsahibiGol)#2820L]\n            +- Expand [ArrayBuffer(Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#2822, Evsahibi#2823, 0), ArrayBuffer(Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#2822, null, 1), ArrayBuffer(Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, null, null, 3)], [Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#2824, Evsahibi#2825, spark_grouping_id#2821]\n               +- Project [Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#93 AS Sehir#2822, Evsahibi#89 AS Evsahibi#2823]\n                  +- Project [Year#10 AS Yıl#88, Home team name#15 AS Evsahibi#89, Home Team Goals#16 AS EvsahibiGol#90, Away Team Goals#17 AS DeplasmanGol#91, Away Team Name#18 AS Deplasman#92, City#14 AS sehir#93]\n                     +- Relation[Year#10,Datetime#11,Stage#12,Stadium#13,City#14,Home Team Name#15,Home Team Goals#16,Away Team Goals#17,Away Team Name#18,Win conditions#19,Attendance#20,Half-time Home Goals#21,Half-time Away Goals#22,Referee#23,Assistant 1#24,Assistant 2#25,RoundID#26,MatchID#27,Home Team Initials#28,Away Team Initials#29] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:120)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:104)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:172)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:178)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:65)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3300)\n\tat org.apache.spark.sql.Dataset.sortInternal(Dataset.scala:3288)\n\tat org.apache.spark.sql.Dataset.sort(Dataset.scala:1177)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-14b92fe897af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrolledUpDf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sehir\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Evsahibi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EvsahibiGol\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sehir\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Evsahibi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EvsahibiGol\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \"\"\"\n\u001b[0;32m--> 978\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`EvsahibiGol`' given input columns: [Sehir, Evsahibi];;\\n'Sort ['EvsahibiGol ASC NULLS FIRST], true\\n+- AnalysisBarrier\\n      +- Project [Sehir#2824, Evsahibi#2825]\\n         +- Aggregate [Sehir#2824, Evsahibi#2825, spark_grouping_id#2821], [Sehir#2824, Evsahibi#2825, sum(cast(EvsahibiGol#90 as bigint)) AS sum(EvsahibiGol)#2820L]\\n            +- Expand [ArrayBuffer(Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#2822, Evsahibi#2823, 0), ArrayBuffer(Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#2822, null, 1), ArrayBuffer(Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, null, null, 3)], [Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#2824, Evsahibi#2825, spark_grouping_id#2821]\\n               +- Project [Yıl#88, Evsahibi#89, EvsahibiGol#90, DeplasmanGol#91, Deplasman#92, sehir#93, Sehir#93 AS Sehir#2822, Evsahibi#89 AS Evsahibi#2823]\\n                  +- Project [Year#10 AS Yıl#88, Home team name#15 AS Evsahibi#89, Home Team Goals#16 AS EvsahibiGol#90, Away Team Goals#17 AS DeplasmanGol#91, Away Team Name#18 AS Deplasman#92, City#14 AS sehir#93]\\n                     +- Relation[Year#10,Datetime#11,Stage#12,Stadium#13,City#14,Home Team Name#15,Home Team Goals#16,Away Team Goals#17,Away Team Name#18,Win conditions#19,Attendance#20,Half-time Home Goals#21,Half-time Away Goals#22,Referee#23,Assistant 1#24,Assistant 2#25,RoundID#26,MatchID#27,Home Team Initials#28,Away Team Initials#29] csv\\n\""
     ]
    }
   ],
   "source": [
    "rolledUpDf=df.rollup(\"Sehir\",\"Evsahibi\").agg(sum(\"EvsahibiGol\")).\\\n",
    "select(\"Sehir\",\"Evsahibi\").orderBy(\"EvsahibiGol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rolledUpDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-8a14851c88f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrolledUpDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rolledUpDf' is not defined"
     ]
    }
   ],
   "source": [
    "rolledUpDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Country`' given input columns: [Win conditions, Home Team Name, Datetime, date, Away Team Name, RoundID, Away Team Goals, Assistant 2, Stadium, Referee, Half-time Away Goals, Half-time Home Goals, Year, Attendance, Away Team Initials, City, Home Team Goals, MatchID, Assistant 1, Home Team Initials, Stage];;\\n'Aggregate [cube(Date#2308, 'Country)], [Date#2308, 'Country, sum('Quantity) AS sum(Quantity)#2486]\\n+- Project [Year#10, Datetime#11, Stage#12, Stadium#13, City#14, Home Team Name#15, Home Team Goals#16, Away Team Goals#17, Away Team Name#18, Win conditions#19, Attendance#20, Half-time Home Goals#21, Half-time Away Goals#22, Referee#23, Assistant 1#24, Assistant 2#25, RoundID#26, MatchID#27, Home Team Initials#28, Away Team Initials#29, to_date('Datetime, Some(MM/d/yyy H:mm)) AS date#2308]\\n   +- Relation[Year#10,Datetime#11,Stage#12,Stadium#13,City#14,Home Team Name#15,Home Team Goals#16,Away Team Goals#17,Away Team Name#18,Win conditions#19,Attendance#20,Half-time Home Goals#21,Half-time Away Goals#22,Referee#23,Assistant 1#24,Assistant 2#25,RoundID#26,MatchID#27,Home Team Initials#28,Away Team Initials#29] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o791.agg.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Country`' given input columns: [Win conditions, Home Team Name, Datetime, date, Away Team Name, RoundID, Away Team Goals, Assistant 2, Stadium, Referee, Half-time Away Goals, Half-time Home Goals, Year, Attendance, Away Team Initials, City, Home Team Goals, MatchID, Assistant 1, Home Team Initials, Stage];;\n'Aggregate [cube(Date#2308, 'Country)], [Date#2308, 'Country, sum('Quantity) AS sum(Quantity)#2486]\n+- Project [Year#10, Datetime#11, Stage#12, Stadium#13, City#14, Home Team Name#15, Home Team Goals#16, Away Team Goals#17, Away Team Name#18, Win conditions#19, Attendance#20, Half-time Home Goals#21, Half-time Away Goals#22, Referee#23, Assistant 1#24, Assistant 2#25, RoundID#26, MatchID#27, Home Team Initials#28, Away Team Initials#29, to_date('Datetime, Some(MM/d/yyy H:mm)) AS date#2308]\n   +- Relation[Year#10,Datetime#11,Stage#12,Stadium#13,City#14,Home Team Name#15,Home Team Goals#16,Away Team Goals#17,Away Team Name#18,Win conditions#19,Attendance#20,Half-time Home Goals#21,Half-time Away Goals#22,Referee#23,Assistant 1#24,Assistant 2#25,RoundID#26,MatchID#27,Home Team Initials#28,Away Team Initials#29] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:120)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:104)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:72)\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:226)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-dd56d4f8fefb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cube\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdfNoNull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Quantity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sum(Quantity)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             jdf = self._jgd.agg(exprs[0]._jc,\n\u001b[0;32m---> 93\u001b[0;31m                                 _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`Country`' given input columns: [Win conditions, Home Team Name, Datetime, date, Away Team Name, RoundID, Away Team Goals, Assistant 2, Stadium, Referee, Half-time Away Goals, Half-time Home Goals, Year, Attendance, Away Team Initials, City, Home Team Goals, MatchID, Assistant 1, Home Team Initials, Stage];;\\n'Aggregate [cube(Date#2308, 'Country)], [Date#2308, 'Country, sum('Quantity) AS sum(Quantity)#2486]\\n+- Project [Year#10, Datetime#11, Stage#12, Stadium#13, City#14, Home Team Name#15, Home Team Goals#16, Away Team Goals#17, Away Team Name#18, Win conditions#19, Attendance#20, Half-time Home Goals#21, Half-time Away Goals#22, Referee#23, Assistant 1#24, Assistant 2#25, RoundID#26, MatchID#27, Home Team Initials#28, Away Team Initials#29, to_date('Datetime, Some(MM/d/yyy H:mm)) AS date#2308]\\n   +- Relation[Year#10,Datetime#11,Stage#12,Stadium#13,City#14,Home Team Name#15,Home Team Goals#16,Away Team Goals#17,Away Team Name#18,Win conditions#19,Attendance#20,Half-time Home Goals#21,Half-time Away Goals#22,Referee#23,Assistant 1#24,Assistant 2#25,RoundID#26,MatchID#27,Home Team Initials#28,Away Team Initials#29] csv\\n\""
     ]
    }
   ],
   "source": [
    "#cube\n",
    "dfNoNull.cube(\"Date\",\"Country\").agg(sum(col(\"Quantity\"))).select(\"Date\",\"Country\",\"sum(Quantity)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Country`' given input columns: [Win conditions, Home Team Name, Datetime, date, Away Team Name, RoundID, Away Team Goals, Assistant 2, Stadium, Referee, Half-time Away Goals, Half-time Home Goals, Year, Attendance, Away Team Initials, City, Home Team Goals, MatchID, Assistant 1, Home Team Initials, Stage];;\\n'Project ['Country]\\n+- AnalysisBarrier\\n      +- Project [Year#10, Datetime#11, Stage#12, Stadium#13, City#14, Home Team Name#15, Home Team Goals#16, Away Team Goals#17, Away Team Name#18, Win conditions#19, Attendance#20, Half-time Home Goals#21, Half-time Away Goals#22, Referee#23, Assistant 1#24, Assistant 2#25, RoundID#26, MatchID#27, Home Team Initials#28, Away Team Initials#29, to_date('Datetime, Some(MM/d/yyy H:mm)) AS date#2308]\\n         +- Relation[Year#10,Datetime#11,Stage#12,Stadium#13,City#14,Home Team Name#15,Home Team Goals#16,Away Team Goals#17,Away Team Name#18,Win conditions#19,Attendance#20,Half-time Home Goals#21,Half-time Away Goals#22,Referee#23,Assistant 1#24,Assistant 2#25,RoundID#26,MatchID#27,Home Team Initials#28,Away Team Initials#29] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o866.pivot.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Country`' given input columns: [Win conditions, Home Team Name, Datetime, date, Away Team Name, RoundID, Away Team Goals, Assistant 2, Stadium, Referee, Half-time Away Goals, Half-time Home Goals, Year, Attendance, Away Team Initials, City, Home Team Goals, MatchID, Assistant 1, Home Team Initials, Stage];;\n'Project ['Country]\n+- AnalysisBarrier\n      +- Project [Year#10, Datetime#11, Stage#12, Stadium#13, City#14, Home Team Name#15, Home Team Goals#16, Away Team Goals#17, Away Team Name#18, Win conditions#19, Attendance#20, Half-time Home Goals#21, Half-time Away Goals#22, Referee#23, Assistant 1#24, Assistant 2#25, RoundID#26, MatchID#27, Home Team Initials#28, Away Team Initials#29, to_date('Datetime, Some(MM/d/yyy H:mm)) AS date#2308]\n         +- Relation[Year#10,Datetime#11,Stage#12,Stadium#13,City#14,Home Team Name#15,Home Team Goals#16,Away Team Goals#17,Away Team Name#18,Win conditions#19,Attendance#20,Half-time Home Goals#21,Half-time Away Goals#22,Referee#23,Assistant 1#24,Assistant 2#25,RoundID#26,MatchID#27,Home Team Initials#28,Away Team Initials#29] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:120)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:104)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3295)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1307)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1325)\n\tat org.apache.spark.sql.RelationalGroupedDataset.pivot(RelationalGroupedDataset.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-5650b9acc4da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#pivot make the row the column,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdfWithDate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpivoted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdfWithDate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpivoted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pivoted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36mpivot\u001b[0;34m(self, pivot_col, values)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mjgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mjgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`Country`' given input columns: [Win conditions, Home Team Name, Datetime, date, Away Team Name, RoundID, Away Team Goals, Assistant 2, Stadium, Referee, Half-time Away Goals, Half-time Home Goals, Year, Attendance, Away Team Initials, City, Home Team Goals, MatchID, Assistant 1, Home Team Initials, Stage];;\\n'Project ['Country]\\n+- AnalysisBarrier\\n      +- Project [Year#10, Datetime#11, Stage#12, Stadium#13, City#14, Home Team Name#15, Home Team Goals#16, Away Team Goals#17, Away Team Name#18, Win conditions#19, Attendance#20, Half-time Home Goals#21, Half-time Away Goals#22, Referee#23, Assistant 1#24, Assistant 2#25, RoundID#26, MatchID#27, Home Team Initials#28, Away Team Initials#29, to_date('Datetime, Some(MM/d/yyy H:mm)) AS date#2308]\\n         +- Relation[Year#10,Datetime#11,Stage#12,Stadium#13,City#14,Home Team Name#15,Home Team Goals#16,Away Team Goals#17,Away Team Name#18,Win conditions#19,Attendance#20,Half-time Home Goals#21,Half-time Away Goals#22,Referee#23,Assistant 1#24,Assistant 2#25,RoundID#26,MatchID#27,Home Team Initials#28,Away Team Initials#29] csv\\n\""
     ]
    }
   ],
   "source": [
    "#pivot make the row the column,\n",
    "dfWithDate.groupBy(\"Date\").sum()\n",
    "pivoted=dfWithDate.groupBy(\"Date\").pivot(\"Country\").sum()\n",
    "pivoted.createOrReplaceTempView(\"pivoted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pivoted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-84de4fdd8983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpivoted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date>'2011-12-05'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"USA_sum(UnitPrice)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pivoted' is not defined"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date>'2011-12-05'\").select(\"date\",\"USA_sum(UnitPrice)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
